{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'deepgeo.common.geofunctions' from '../src/deepgeo/common/geofunctions.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from importlib import reload\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.insert(0, '../src')\n",
    "import deepgeo.dataset.data_augment as dtaug\n",
    "import deepgeo.dataset.utils as dsutils \n",
    "import deepgeo.common.geofunctions as gf\n",
    "import deepgeo.networks.model_builder as mb\n",
    "\n",
    "reload(dtaug)\n",
    "reload(dsutils)\n",
    "reload(mb)\n",
    "reload(gf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load input Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR = os.path.join(os.path.abspath(os.path.dirname('__file__')), '../', 'data_real', 'generated')\n",
    "network = 'unet'\n",
    "DATA_DIR = '/home/raian/doutorado/Dados/generated'\n",
    "DATASET_FILE = os.path.join(DATA_DIR, 'new_dataset_286x286_timesstack-2013-2017.npz')\n",
    "\n",
    "# model_dir = os.path.join(DATA_DIR, 'tf_logs', network,\n",
    "#                          'test_%s_%s' % (network, datetime.now().strftime('%d_%m_%Y-%H_%M_%S')))\n",
    "# model_dir = '/home/raian/doutorado/deepgeo/data_real/generated/tf_logs/test_debug'\n",
    "model_dir = os.path.join(DATA_DIR, 'tf_logs', 'experiments', 'unet', 'test_unet_29_04_2019-12_51_06')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf {model_dir}/*\n",
    "# !ls -al {model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset between train, test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded:\n",
      "  -> Images:  7872\n",
      "  -> Labels:  7872\n",
      "  -> Classes:  3\n",
      "Images shape:  (286, 286, 10)  - DType:  float32\n",
      "Labels shape:  (286, 286, 1)  - DType:  int32\n",
      "Classes:  ['no_data' 'not_deforestation' 'deforestation']\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load(DATASET_FILE)\n",
    "\n",
    "print(\"Data Loaded:\")\n",
    "print(\"  -> Images: \", len(dataset[\"images\"]))\n",
    "print(\"  -> Labels: \", len(dataset[\"labels\"]))\n",
    "print(\"  -> Classes: \", len(dataset[\"classes\"]))\n",
    "\n",
    "print(\"Images shape: \", dataset[\"images\"][0].shape, \" - DType: \", dataset[\"images\"][0].dtype)\n",
    "print(\"Labels shape: \", dataset[\"labels\"][0].shape, \" - DType: \", dataset[\"labels\"][0].dtype)\n",
    "print('Classes: ', dataset['classes'])\n",
    "# print(\"UNIQUE LABELS: \", np.unique(dataset[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted dataset:\n",
      "  -> Train images:  (4723, 286, 286, 10)\n",
      "  -> Test images:  (1574, 286, 286, 10)\n",
      "  -> Validation images:  (1575, 286, 286, 10)\n",
      "  -> Train Labels:  (4723, 286, 286, 1)\n",
      "  -> Test Labels:  (1574, 286, 286, 1)\n",
      "  -> Validation Labels:  (1575, 286, 286, 1)\n"
     ]
    }
   ],
   "source": [
    "train_images, test_images, valid_images, train_labels, test_labels, valid_labels = dsutils.split_dataset(dataset,\n",
    "                                                                                                         perc_test=20,\n",
    "                                                                                                         perc_val=20)\n",
    "\n",
    "print(\"Splitted dataset:\")\n",
    "print(\"  -> Train images: \", train_images.shape)\n",
    "print(\"  -> Test images: \", test_images.shape)\n",
    "print(\"  -> Validation images: \", valid_images.shape)\n",
    "print(\"  -> Train Labels: \", train_labels.shape)\n",
    "print(\"  -> Test Labels: \", test_labels.shape)\n",
    "print(\"  -> Validation Labels: \", valid_labels.shape)\n",
    "total_train_chips = train_images.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = [90, 180, 270]\n",
    "rotated_imgs = dtaug.rotate_images(train_images, angles)\n",
    "flipped_imgs = dtaug.flip_images(train_images)\n",
    "\n",
    "train_images = np.concatenate((train_images, rotated_imgs, flipped_imgs))\n",
    "# train_images = np.concatenate((train_images, ))\n",
    "\n",
    "rotated_lbls = dtaug.rotate_images(train_labels, angles)\n",
    "flipped_lbls = dtaug.flip_images(train_labels)\n",
    "\n",
    "train_labels = np.concatenate((train_labels, rotated_lbls, flipped_lbls))\n",
    "# train_labels = np.concatenate((train_labels, )).astype(dtype=np.int32)\n",
    "\n",
    "print('Data Augmentation Applied:')\n",
    "print('  -> Train Images: ', train_images.shape)\n",
    "print('  -> Train Labels: ', train_labels.shape)\n",
    "print('  -> Test Images: ', test_images.shape)\n",
    "print('  -> Test Labels: ', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weights_mean_proportion(batch_array, classes, classes_zero=['no_data']):\n",
    "    values, count = np.unique(batch_array, return_counts=True)\n",
    "    count = [count[i] if classes[i] not in classes_zero else 0 for i in range(0, len(count))]\n",
    "    total = sum(count)\n",
    "    proportions = [i / total for i in count]\n",
    "    mean_prop = sum(proportions)/ (len(proportions) - len(classes_zero))\n",
    "    weights = [mean_prop / i if i != 0 else 0 for i in proportions]\n",
    "    return weights\n",
    "\n",
    "weights = compute_weights_mean_proportion(train_labels, dataset['classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot chips to check Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "def plot_chips_and_labels(imgs, lbls, pos):\n",
    "    plt.figure(figsize=(30,30))\n",
    "    plt.subplots(nrows=10, ncols=10)\n",
    "    count = 1\n",
    "    plt.subplot(1, 2, count)\n",
    "    plt.title(str(pos))\n",
    "    plt.axis('off')\n",
    "    count += 1\n",
    "    plt.imshow(imgs[pos][:,:,[8, 7, 6]], interpolation='nearest')\n",
    "\n",
    "    colorMap = ListedColormap(['white', 'green', 'red'])\n",
    "    plt.subplot(1, 2, count)\n",
    "    #     plt.title('pos %d' % (pos))\n",
    "    plt.title(str(np.unique(lbls[pos])))\n",
    "    plt.axis('off')\n",
    "    count += 1\n",
    "    plt.imshow(lbls[pos][:,:,0], interpolation='nearest', cmap=colorMap)\n",
    "\n",
    "\n",
    "pos = random.randrange(0, total_train_chips)\n",
    "pos_1 = pos\n",
    "\n",
    "plot_chips_and_labels(train_images, train_labels, pos)\n",
    "pos += total_train_chips\n",
    "plot_chips_and_labels(train_images, train_labels, pos)\n",
    "pos += total_train_chips\n",
    "plot_chips_and_labels(train_images, train_labels, pos)\n",
    "pos += total_train_chips\n",
    "plot_chips_and_labels(train_images, train_labels, pos)\n",
    "pos += ((total_train_chips - pos_1) + (3 * pos_1))\n",
    "plot_chips_and_labels(train_images, train_labels, pos)\n",
    "pos += 1\n",
    "plot_chips_and_labels(train_images, train_labels, pos)\n",
    "pos += 1\n",
    "plot_chips_and_labels(train_images, train_labels, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, count = np.unique(train_labels, return_counts=True)\n",
    "print(\"UNIQUE: \", values, count)\n",
    "\n",
    "defor_proportion = count[1] / (count[0] + count[1])\n",
    "non_defor_proportion = count[0] / (count[0] + count[1])\n",
    "print('Deforestation Proportion: ', defor_proportion)\n",
    "print('Non deforestation Proportion: ', non_defor_proportion)\n",
    "\n",
    "print('Ratio: ', non_defor_proportion / defor_proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_proportion = (defor_proportion + non_defor_proportion) / 2\n",
    "print(\"Median Proportion: \", mean_proportion)\n",
    "weight_defor = mean_proportion / defor_proportion\n",
    "weight_non_defor = mean_proportion / non_defor_proportion\n",
    "\n",
    "print('Weights: [', weight_defor, ', ', weight_non_defor, ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'epochs': 600,\n",
    "    'batch_size': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'learning_rate_decay': True,\n",
    "    'decay_rate': 0.1,\n",
    "    'decay_steps': 245,\n",
    "    'l2_reg_rate': 0.5,\n",
    "    'var_scale_factor': 2.0,\n",
    "    'chips_tensorboard': 2,\n",
    "    'dropout_rate': 0.5,\n",
    "    'fusion': 'early',\n",
    "    'loss_func': 'weighted_crossentropy',\n",
    "    'class_weights': [0.01, 0.99],\n",
    "    'num_classes': len(dataset['classes']),\n",
    "    'class_names': dataset['classes'],\n",
    "     'bands_plot': [6, 7, 8]\n",
    "}\n",
    "reload(mb)\n",
    "model = mb.ModelBuilder(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.train(train_images, test_images, train_labels, test_labels, params, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/raian/doutorado/Dados/generated/tf_logs/experiments/unet/test_unet_29_04_2019-12_51_06/model.ckpt-96500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "total: 1575\n",
      "shape: (1575, 10000)\n",
      "shape labels: (1575, 10000)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multilabel-indicator and multiclass-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-2e9125815dbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/doutorado/deepgeo/src/deepgeo/networks/model_builder.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self, images, expect_labels, params, model_dir)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shape:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shape labels:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0mf1_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m    712\u001b[0m     return fbeta_score(y_true, y_pred, 1, labels=labels,\n\u001b[1;32m    713\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m                        sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m    826\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f-score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    829\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and multiclass-multioutput targets"
     ]
    }
   ],
   "source": [
    "model.validate(valid_images, valid_labels, params, model_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
